/u/ssoma1/.local/lib/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
/u/ssoma1/.local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W801 02:36:35.941660326 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
wandb: Currently logged in as: msharif (msharif-iowa-state-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /u/ssoma1/mahdi/Isomorphic-Pruning/wandb/run-20250801_023636-h17017qp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run results_pruned_ConvNext_Small_final_pruned_convnext_small_imagenet_rev4_ratio0.282_full.pt
wandb: ‚≠êÔ∏è View project at https://wandb.ai/msharif-iowa-state-university/Pruning
wandb: üöÄ View run at https://wandb.ai/msharif-iowa-state-university/Pruning/runs/h17017qp
/u/ssoma1/mahdi/Isomorphic-Pruning/train_ConvNeXt.py:397: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if args.amp else None
/u/ssoma1/mahdi/Isomorphic-Pruning/train_ConvNeXt.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=scaler is not None):
/u/ssoma1/.local/lib/python3.9/site-packages/torch/autograd/graph.py:824: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [496, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [496, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:328.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
slurmstepd: error: *** JOB 11277965 ON gpua056 CANCELLED AT 2025-08-01T03:36:42 DUE TO TIME LIMIT ***
